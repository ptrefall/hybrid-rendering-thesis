\part{Conclusion}


This thesis was originally supposed to explore volume rendering with the OptiX raytracing framework, as this was a proposed assignment by our project advisor. We spent the good half of February 2012 researching volume rendering. We were thinking of exploring compositing volume graphics with typical solid-surface realtime graphics. Pål had for a long time wanted to look closer at tilebased deferred rendering, and we decided to study the ways this relatively new technique could be combined with realtime raytracing. Both deferred rendering with rasterization and raytracing are relatively old techniques that have recently become relevant for realtime graphics. Deferred pipelines became popular around 2004-2005 when GPUs finally had enough memory to allow multiple G-buffers to be held in-memory. Raytracing on GPUs has become easy to program thanks to languages like OpenCL and CUDA. There are visual effects that can only be accomplished accurately with raytracing (reflections and refractions), also certain types of geometry lend themselves naturally to raytracing (signed distance fields and spline-surfaces). Since a deferred renderer outputs much of the information needed (surface position and normal) to start secondary rays, we thought combining the two could be advantageous.

Pål quickly created an OpenGL based deferred renderer, and the idea was to plug OptiX into this, as it seemingly had good interopability with OpenGL. Sadly, we've spent a lot of time fighting crashes caused by OptiX and OpenGL interop. Having to wait for minutes to recover from display driver crashes sapped energy and willpower. The retained-mode API-behaviour of OptiX hasn't helped when integrating it into our render-stage based framework either. OptiX isn't a bad library, it does what it claims very well, but it needs more users. For that to happen, it has to become more open and work on other vendors hardware. In hindsight, we should have written the raytracer ourselves.

Midway through the project, we decided we needed a scene that could showcase the properties of a hybrid. The BART project seemed like a perfect fit. One of the sample scenes contains mostly diffuse shaded geometry and some translucent geometry. It took some effort getting the BART parser into a workable state. The point of using BART was somewhat lost, as we didn't implement the animation part of it, so, we can't compare our renderer against other implementations.

We didn't have a clear plan of distributing work between the rasterizer and the raytracer. Should only parts of the screen be raytraced? Should the raytracer handle soft omni-directional shadows? Should the rasterizer only handle its G-buffer creation, and OptiX the final shading? We tried to do too much. Much time was spent on details such as C++ design and implementing a ``perfect'' deferred pipeline, BART loader and getting to grips with the OptiX programming model.

TODO write something positive.